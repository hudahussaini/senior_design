{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sameerc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sameerc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sameerc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /nfs/turbo/si-acastel/expert_field_project/full_pdfs_by_author/huesmann/xml.pdf: Could not read malformed PDF file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 60\n",
      "INFO:guidedlda:vocab_size: 23425\n",
      "INFO:guidedlda:n_words: 193123\n",
      "INFO:guidedlda:n_topics: 9\n",
      "INFO:guidedlda:n_iter: 100\n",
      "INFO:guidedlda:<0> log likelihood: -2135560\n",
      "INFO:guidedlda:<20> log likelihood: -1682116\n",
      "INFO:guidedlda:<40> log likelihood: -1647487\n",
      "INFO:guidedlda:<60> log likelihood: -1632869\n",
      "INFO:guidedlda:<80> log likelihood: -1623172\n",
      "INFO:guidedlda:<99> log likelihood: -1619860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: aggressivebehavior, crossref, childdevelopment, figure, table, etal, aggression, xfebruary\n",
      "Topic 1: arousal, memory, film, clip, recall, subject, model, amnesia\n",
      "Topic 2: time, item, positive, negative, model, would, size, rate\n",
      "Topic 3: youth, trait, report, teacher, composite, score, sample, model\n",
      "Topic 4: aggression, child, parent, factor, participant, huesmann, family, measure\n",
      "Topic 5: violence, violent, medium, aggressive, behavior, effect, game, study\n",
      "Topic 6: violence, exposure, author, child, wave, political, manuscript, israeli\n",
      "Topic 7: data, table, study, problem, result, learning, however, used\n",
      "Topic 8: aggression, child, belief, social, behavior, huesmann, aggressive, normative\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is developed and tested with Python 3.6.13 environment.\n",
    "\n",
    "Install the necessary libraries using pip to make sure the Python environment matches the versions\n",
    "\n",
    "pip install PyPDF2==1.26.0\n",
    "pip install guidedlda\n",
    "pip install numpy==1.19.5\n",
    "pip install nltk==3.5\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "import PyPDF2\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set the path to your PDF folder\n",
    "all_pdf_folder_path = '/nfs/turbo/si-acastel/expert_field_project/full_pdfs_by_author/huesmann'\n",
    "path_all_pdf_folder_path = pathlib.Path(all_pdf_folder_path)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfFileReader(file, strict=False)\n",
    "            text = ''\n",
    "            for page in range(reader.numPages):\n",
    "                try:\n",
    "                    text += reader.getPage(page).extractText()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text from page {page} of {pdf_path}: {str(e)}\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and 4 <= len(token) <= 20]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Extract text from all PDFs\n",
    "all_texts = []\n",
    "for pdf_file in path_all_pdf_folder_path.glob('*.pdf'):\n",
    "    text = extract_text_from_pdf(str(pdf_file))\n",
    "    if text:\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        all_texts.append(preprocessed_text)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for text in all_texts:\n",
    "    words = text.split()\n",
    "    vocab.update(words)\n",
    "\n",
    "vocab = list(vocab)\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Create document-term matrix\n",
    "X = np.zeros((len(all_texts), len(vocab)), dtype=np.int64)\n",
    "for i, text in enumerate(all_texts):\n",
    "    word_counts = Counter(text.split())\n",
    "    for word, count in word_counts.items():\n",
    "        if word in word2id:\n",
    "            X[i, word2id[word]] = int(count)\n",
    "\n",
    "# Define optimized seed topics based on the author list\n",
    "seed_topics = {\n",
    "    0: ['survey', 'methodology', 'statistics', 'sampling', 'data'],\n",
    "    1: ['politics', 'policy', 'government', 'election', 'democracy'],\n",
    "    2: ['health', 'medicine', 'wellbeing', 'psychology', 'behavior'],\n",
    "    3: ['economics', 'finance', 'market', 'income', 'wealth'],\n",
    "    4: ['sociology', 'demography', 'family', 'community', 'social'],\n",
    "    5: ['education', 'learning', 'school', 'academic', 'student'],\n",
    "    6: ['environment', 'climate', 'sustainability', 'ecology', 'urban'],\n",
    "    7: ['communication', 'media', 'technology', 'internet', 'social media'],\n",
    "    8: ['international', 'global', 'culture', 'migration', 'development']\n",
    "}\n",
    "\n",
    "# Create seed_topic_dict\n",
    "seed_topic_dict = {}\n",
    "for topic_id, words in seed_topics.items():\n",
    "    for word in words:\n",
    "        if word in word2id:\n",
    "            seed_topic_dict[word2id[word]] = topic_id\n",
    "\n",
    "# Apply Guided LDA\n",
    "model = guidedlda.GuidedLDA(n_topics=9, n_iter=100, random_state=7, refresh=20)\n",
    "model.fit(X, seed_topics=seed_topic_dict, seed_confidence=0.15)\n",
    "\n",
    "# Print top words for each topic\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(model.topic_word_):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print(f'Topic {i}: {\", \".join(topic_words)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
