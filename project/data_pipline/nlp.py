# -*- coding: utf-8 -*-
"""Copy of baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105rVJuS7ND60z7AogTxt_4wHd-4_-tdw

### Envirorment Set-Up

conda create --name BERTopic
conda activate BERTopic
conda install -c anaconda ipykernel
python -m ipykernel install --user --name=BERTopic
pip install bertopic
pip install nltk
pip install pymupdf

### Dependencies
"""

import os
import fitz  # PyMuPDF
import nltk
import string
import sklearn
from bertopic import BERTopic
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.pipeline import Pipeline

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

"""### Text Processing"""

def read_pdf(file_path):
    text = ""
    with fitz.open(file_path) as doc:
        for page in doc:
            text += page.get_text()
    return text

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercase
    tokens = [word.lower() for word in tokens]

    # Remove punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # Remove non-alphabetic tokens
    words = [word for word in stripped if word.isalpha()]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    return " ".join(words)

def find_files_in_current_directory():
    current_directory = os.getcwd()
    files = [f for f in os.listdir(current_directory) if os.path.isfile(os.path.join(current_directory, f))]
    return files

# Call the function to get a list of file names
file_names = find_files_in_current_directory()

len(file_names)

def create_docs(file_names):
    docs = []
    for file in file_names:
        try:
            text = read_pdf(file)
            preprocessed_text = preprocess_text(text)
            docs.append(preprocessed_text)
        except:
            pass

    return docs

docs = create_docs(file_names)

"""### Modeling"""

BERTopic().get_params()

from sentence_transformers import SentenceTransformer, util

sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=True)

model = BERTopic().fit(docs, embeddings)

"""model = BERTopic(n_gram_range=(1,2), nr_topics=6)
topics, probs = model.fit_transform(docs)
"""

model.get_topic_freq()

model.get_topic_info()

#non responce -> topic disamp





