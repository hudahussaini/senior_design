# -*- coding: utf-8 -*-
"""Copy of baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105rVJuS7ND60z7AogTxt_4wHd-4_-tdw

### Envirorment Set-Up

conda create --name BERTopic
conda activate BERTopic
conda install -c anaconda ipykernel
python -m ipykernel install --user --name=BERTopic
pip install bertopic
pip install nltk
pip install pymupdf

### Dependencies
"""

import os
import PyPDF2  # PyMuPDF
import nltk
import string
import sklearn
from bertopic import BERTopic
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.pipeline import Pipeline

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')

"""### Text Processing"""

def read_pdf(file_path):
    """Reads a PDF file and extracts text from all pages.

    Args:
        file_path (str): Path to the PDF file.

    Returns:
        str: Extracted text from the PDF.
    """

    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page_num in range(len(reader.pages)):
                page = reader.pages[page_num]
                text += page.extract_text()
            return text
    except (FileNotFoundError, PermissionError, PyPDF2.errors.PdfReadError) as e:
        print(f"Error reading PDF file: {file_path}, {e}")
        return ""  # Or raise an exception if preferred

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercase
    tokens = [word.lower() for word in tokens]

    # Remove punctuation
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]

    # Remove non-alphabetic tokens
    words = [word for word in stripped if word.isalpha()]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if not word in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    return " ".join(words)

def find_files_in_current_directory():
    current_directory = os.getcwd()
    files = [f for f in os.listdir(current_directory) if os.path.isfile(os.path.join(current_directory, f))]
    return files

def create_docs(file_names):
    docs = []
    for file in file_names:
        try:
            text = read_pdf(file)
            preprocessed_text = preprocess_text(text)
            docs.append(preprocessed_text)
        except Exception as e:
            print(f"Error processing file {file}: {e}")
            # Optionally, you can log the error or skip the file here

    if not docs:
        # Handle the case where no files were processed successfully
        print("No documents created due to processing errors.")
    return docs







