{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from nltk.corpus import stopwords\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from gensim import corpora, models\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import pyLDAvis\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# import pyLDAvis.gensim\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# nltk.download('punkt_tab')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#TODO: Make it pick out stop words somehow - not a priority\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords, wordnet\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from nltk.corpus import stopwords\n",
    "# from gensim import corpora, models\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = stopwords.words('english')\n",
    "# nltk.download('punkt_tab')\n",
    "#TODO: Make it pick out stop words somehow - not a priority\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def preprocess_with_bigrams(docs, min_word_length=3, remove_stopwords=True, \n",
    "                          lemmatize=True, min_count=5, threshold=100):\n",
    "    \"\"\"\n",
    "    Preprocess documents and detect common bigram phrases.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of document strings\n",
    "        min_word_length (int): Minimum word length to keep\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        lemmatize (bool): Whether to lemmatize words\n",
    "        min_count (int): Minimum frequency for bigram detection\n",
    "        threshold (float): Threshold for bigram detection scoring\n",
    "        \n",
    "    Returns:\n",
    "        list: List of processed documents with bigrams\n",
    "    \"\"\"\n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger_eng')\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Some NLTK downloads failed: {str(e)}\")\n",
    "    \n",
    "    # Initialize tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stops = {'from', 'subject', 're', 'edu', 'use', 'amanda', 'kowalski', 'nber'}\n",
    "    stop_words.update(custom_stops)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Remove URLs, special characters, and extra whitespace.\"\"\"\n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+|\\S+@\\S+', '', text)\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        return ' '.join(text.split()).strip()\n",
    "    \n",
    "    def get_wordnet_pos(tag):\n",
    "        \"\"\"Convert Penn Treebank tags to WordNet POS tags.\"\"\"\n",
    "        tag_map = {\n",
    "            'J': wordnet.ADJ,\n",
    "            'V': wordnet.VERB,\n",
    "            'N': wordnet.NOUN,\n",
    "            'R': wordnet.ADV\n",
    "        }\n",
    "        return tag_map.get(tag[0], wordnet.NOUN)\n",
    "    \n",
    "    # First pass: basic preprocessing\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        # Clean and tokenize\n",
    "        clean_doc = clean_text(doc.lower())\n",
    "        tokens = word_tokenize(clean_doc)\n",
    "        \n",
    "        # Process tokens\n",
    "        if lemmatize:\n",
    "            # POS tagging for better lemmatization\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            processed_tokens = []\n",
    "            \n",
    "            for token, pos in pos_tags:\n",
    "                if (token.isalpha() and \n",
    "                    len(token) >= min_word_length and\n",
    "                    (not remove_stopwords or token not in stop_words)):\n",
    "                    lemma = lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "                    processed_tokens.append(lemma)\n",
    "        else:\n",
    "            processed_tokens = [\n",
    "                token for token in tokens\n",
    "                if token.isalpha() and \n",
    "                len(token) >= min_word_length and\n",
    "                (not remove_stopwords or token not in stop_words)\n",
    "            ]\n",
    "        \n",
    "        processed_docs.append(processed_tokens)\n",
    "    \n",
    "    # Second pass: detect and add bigrams\n",
    "    bigram = Phrases(processed_docs, \n",
    "                    min_count=min_count,\n",
    "                    threshold=threshold)\n",
    "    bigram_model = Phraser(bigram)\n",
    "    \n",
    "    # Apply bigram detection\n",
    "    docs_with_bigrams = [bigram_model[doc] for doc in processed_docs]\n",
    "    \n",
    "    return docs_with_bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def create_docs(pdf_files):\n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        with fitz.open(pdf_file) as pdf:\n",
    "            text = \"\"\n",
    "            for page_num in range(len(pdf)):\n",
    "                page = pdf.load_page(page_num)\n",
    "                text += page.get_text()\n",
    "            docs.append(text)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_documents(docs):\n",
    "#     processed_docs = []\n",
    "#     for doc in docs:\n",
    "#         tokens = nltk.word_tokenize(doc.lower())\n",
    "#         tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "#         processed_docs.append(tokens)    \n",
    "#     return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hudah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "author_folder_path = '/nfs/turbo/si-acastel/expert_field_project/full_pdfs_by_author/aekowals/'\n",
    "if os.path.isdir(author_folder_path):\n",
    "        pdf_files = [os.path.join(author_folder_path, f) for f in os.listdir(author_folder_path)\n",
    "            if os.path.isfile(os.path.join(author_folder_path, f)) and f.endswith('.pdf')]\n",
    "        docs = create_docs(pdf_files)\n",
    "        # processed_docs = preprocess_documents(docs)\n",
    "        # print(docs)\n",
    "        # print(processed_docs)\n",
    "        processed = preprocess_with_bigrams(\n",
    "        docs,\n",
    "        min_count=2,  # Lower threshold for this small example\n",
    "        threshold=10   # Lower threshold for this small example\n",
    "        )\n",
    "        \n",
    "        # Print a sample of detected bigrams\n",
    "        print(\"\\nProcessed documents with bigrams:\")\n",
    "        for i, doc in enumerate(processed[:2], 1):\n",
    "            print(f\"\\nDoc {i}:\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 30 , Vocab size: 5367 , Num words: 149217\n",
      "Removed top words: ['estimate', 'individual', 'state', 'data', 'reform', 'hospital', 'health_insurance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249533/530351718.py:12: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  hdp.train(0)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "hdp = tp.HDPModel(tw=tp.TermWeight.IDF, min_cf=5, rm_top=7,\n",
    "                 gamma=1, alpha=0.1, initial_k=10, seed=99999)\n",
    "\n",
    "# Add docs to train\n",
    "for vec in processed:\n",
    "    hdp.add_doc(vec)\n",
    "\n",
    "# Initiate MCMC burn-in \n",
    "hdp.burn_in = 100\n",
    "hdp.train(0)\n",
    "print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs, ', Num words:', hdp.num_words)\n",
    "print('Removed top words:', hdp.removed_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nber_working',\n",
       " 'series',\n",
       " 'behavior',\n",
       " 'clinical_trial',\n",
       " 'implications',\n",
       " 'mammography',\n",
       " 'guidelines']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from gensim.models import Phrases\n",
    "# from gensim.models.phrases import Phraser\n",
    "\n",
    "# bigram = Phrases(processed_docs, min_count=5, threshold=100)\n",
    "# bigram_model = Phraser(bigram)\n",
    "# word_bigrams = [bigram_model[w_vec] for w_vec in processed_docs]\n",
    "# word_bigrams[2][:7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
